{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "context = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(X):\n",
    "    X = np.load(X,allow_pickle=True)\n",
    "    data1=[]\n",
    "    for utter in X:\n",
    "        data1.append(np.pad(utter, [(context,context),(0,0)], 'constant', constant_values=(0,0)))\n",
    "    data1 = torch.from_numpy(np.concatenate(data1))\n",
    "    return data1\n",
    "def load(Y):\n",
    "    Y = np.load(Y,allow_pickle=True)\n",
    "    lab=[]\n",
    "    for item in Y:\n",
    "        lab.append(np.pad(item, (context,context), 'constant', constant_values=(0,0)))\n",
    "    labels=torch.from_numpy(np.concatenate(lab))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(X):\n",
    "    X = np.load(X,allow_pickle=True)\n",
    "    lst = []\n",
    "    for item in X:\n",
    "        lst.append(np.pad(np.ones(item.shape[0]).astype(bool), (context,context), constant_values=False, mode='constant'))\n",
    "    lst = np.concatenate(lst)\n",
    "    index = np.array(range(len(lst)))\n",
    "    use_index = lst[index].tolist()\n",
    "    return use_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_load('train.npy')\n",
    "train_labels = load('train_labels.npy')\n",
    "train_index = get_index('train_labels.npy')\n",
    "dev = data_load('dev.npy')\n",
    "dev_labels = load('dev_labels.npy')\n",
    "dev_index = get_index('dev.npy')\n",
    "test = data_load('test.npy')\n",
    "test_index= get_index('test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        X = self.X[index-context:index+context+1].float().reshape(-1) #flatten the input\n",
    "        Y = self.Y[index].long()\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        X = self.X[index-context:index+context+1].float().reshape(-1) #flatten the input\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(data.Sampler):\n",
    "    def __init__(self, data_source, index, train=False):    \n",
    "        super().__init__(data_source)\n",
    "        self.data_source = data_source\n",
    "        self.index = index\n",
    "        self.train = train\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.train:\n",
    "            np.random.shuffle(self.index)\n",
    "            return iter(self.index)\n",
    "        else:\n",
    "            return iter(self.index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "num_workers = 0\n",
    "# Training\n",
    "train_dataset = MyDataset(train, train_labels)\n",
    "\n",
    "sampler_train = Sampler(train_dataset, train_index, train=True)\n",
    "\n",
    "train_loader_args = dict(shuffle=False, batch_size=batch_size, num_workers=num_workers, pin_memory=True, sampler = sampler_train) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=batch_size)\n",
    "train_loader = data.DataLoader(train_dataset, **train_loader_args)\n",
    "\n",
    "# Dev\n",
    "dev_dataset = MyDataset(dev, dev_labels)\n",
    "\n",
    "smapler_dev = Sampler(dev_dataset, dev_index, train=False)\n",
    "\n",
    "dev_loader_args = dict(shuffle=False, batch_size=batch_size, num_workers=num_workers, pin_memory=True, sampler = sampler_dev) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=batch_size)\n",
    "\n",
    "dev_loader = data.DataLoader(dev_dataset, **dev_loader_args)\n",
    "\n",
    "# Test\n",
    "test_dataset = Dataset(test)\n",
    "\n",
    "sampler_test = Sampler(test_dataset, test_index, train=False)\n",
    "\n",
    "test_loader_args = dict(shuffle=False, batch_size=batch_size, num_workers=num_workers, pin_memory=True, sampler = sampler_test) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=batch_size)\n",
    "test_loader = data.DataLoader(test_dataset, **test_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITION\n",
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple_MLP, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Linear(input_size,2048))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(2048))\n",
    "\n",
    "        layers.append(nn.Linear(2048,4096))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(4096))\n",
    "        \n",
    "        layers.append(nn.Linear(4096,2048))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(2048))\n",
    "        \n",
    "        layers.append(nn.Linear(2048,1024))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(0.2))\n",
    "        \n",
    "        layers.append(nn.Linear(1024,512))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(0.2))\n",
    "\n",
    "        layers.append(nn.Linear(512, output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=923, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Dropout(p=0.2, inplace=False)\n",
      "    (15): Linear(in_features=512, out_features=346, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Create the model and define the Loss and Optimizer\n",
    "input_size = 13* (context*2+1)\n",
    "output_size = 346\n",
    "model = Simple_MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function that will train the network for one epoch\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data.to(device)\n",
    "        target = target.to(device) # all data & model on same device\n",
    "\n",
    "        outputs = model(data)\n",
    "        \n",
    "        predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += target.size(0)\n",
    "        correct_predictions += (predicted[1] == target).sum().item()\n",
    "        \n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    acc = (correct_predictions/total_predictions)*100.0\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    print('Training Accuracy: ', acc, '%')\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function that will evaluate our network's performance on the test set\n",
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted[1] == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "Train_loss_lst = []\n",
    "dev_loss_lst = []\n",
    "dev_acc_lst = []\n",
    "MODEL_NAME = 'model'\n",
    "\"\"\" traing the model \"\"\"\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    dev_loss, dev_acc = test_model(model, dev_loader, criterion)\n",
    "    Train_loss_lst.append(train_loss)\n",
    "    dev_loss_lst.append(dev_loss)\n",
    "    dev_acc_lst.append(dev_acc)\n",
    "    print('='*20)\n",
    "    \n",
    "    # may save the training model for future use\n",
    "    if not os.path.exists(\"./model\"):\n",
    "        os.mkdir(\"./model\")\n",
    "\n",
    "    torch.save(model.state_dict(),'/home/unbuntu/11785/model/{}_{}'.format(MODEL_NAME,i))\n",
    "    logging.info('model saved to ./model/{}_{}'.format(MODEL_NAME,i))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "net.load_state_dict(torch.load('/home/unbuntu/11785/model/model_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        outputs = net(x_batch)\n",
    "        \n",
    "        predicted = torch.max(outputs.data, 1)\n",
    "        preds.append(predicted)\n",
    "\n",
    "preds = torch.cat(preds)\n",
    "preds = preds.cpu().numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id = np.array(range(len(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = {'id': Id,\n",
    "       'label': preds,\n",
    "       }\n",
    "df = pd.DataFrame(pre)\n",
    "df.to_csv(\"submit_MLP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c 11-785-fall-20-slack-homework-1-part-2 -f submission.csv -m \"Message\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
